---
title: "R Pirate Pals Twitter Project"
author: "Nathan"
date: "9/11/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(powerlmm)

```

```{r}

p <- study_parameters(n1 = 111111,
                      n2 = 18,
                      icc_pre_subject = .05,
                      var_ratio =  .000018,
                      effect_size = cohend(.4, standardizer = "pretest_SD"))


x <- get_power_table(p, n2 = seq(2, 10, by = 1), var_ratio = c(0), effect_size = cohend(c(.06,.1)))

plot(x)

```



```{r}

library(rtweet)
# Data Wrangling and Visualization
library(glue)
library(cowplot)
library(magrittr)
library(plotly)
library(tidyverse)
library(tidytext)
library(widyr)
# Date & Time Manipulation.
library(hms)
library(lubridate) 
# Text Mining
library(tidytext)
library(tm)
library(wordcloud)
# Network Analysis
library(igraph)
# Network Visualization (D3.js)
library(networkD3)
library(ngram)
# Sentiment analysis
library(syuzhet)
# misc libraries nathan uses
library(ggraph)
library(reshape2)
library(devtools)
library(SnowballC)



library(relativeVariability)
#library(vader)

source("summarySE.R")

```

#Get the tweets

```{r}

#VLtweets <- get_timeline(c("AlterNet", "democracynow", "thedailybeast", "HuffPost", "theintercept",
#                           "jacobinmag", "motherJones","MSNBC", "NewYorker", "thenation", "Slate", 
#                       #    "voxdotcom"), n = 3200)

search_tweets()
```

```{r}

LLtweets <- get_timeline(c("ABC", "theAtlantic", "buzzfeednews", "CBSNews", "CNN", "TheEconomist",
                           "guardiannews", "nbcNews", "nytimes", "politico", "TIME", "washingtonpost", 
                           "AlterNet", "democracynow", "thedailybeast", "HuffPost", "theintercept",
                           "jacobinmag", "motherJones","MSNBC", "NewYorker", "thenation", "Slate", 
                           "voxdotcom"), n = 3200)

```

```{r}

#Centertweets <- get_timeline(c("NPR", "AP", "BBCNews", "csmonitor", "reuters", "thehill", "usatoday", "WSJ", "business", "forbes", "Axios", "businessinsider"), n = 3200)

```

```{r}

LCtweets <- get_timeline(c("reason", "dcexaminer", "WashTimes", "Daily_Press", "foxnewsradio", "MarketWatch", "OANN", "amconmag", "TPostMillennial", "BreitbartNews", 
                           "NRO", "amspectator", "theblaze", "CBNNews", "DailyCaller", "MailOnline", "realDailyWire", "FDRLST", "nypost", "newsmax"), n = 3200)

```

```{r}

#VCtweets <- get_timeline(c("BreitbartNews", "NRO", "amspectator", "theblaze", "CBNNews", "DailyCaller", "MailOnline", "realDailyWire", "FDRLST", "nypost", "newsmax"), n = 3200)

```

#remove https

```{r}

# remove http elements manually

LLtweets$stripped_text <- gsub("http.*","",  LLtweets$text)
LLtweets$stripped_text <- gsub("https.*","", LLtweets$stripped_text)


LCtweets$stripped_text <- gsub("http.*","",  LCtweets$text)
LCtweets$stripped_text <- gsub("https.*","", LCtweets$stripped_text)


```


```{r}

clean_Ctweets <- analysisLC %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text)

clean_Ltweets <- analysisLL %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
```

```{r}

clean_Ctweets_words <- clean_Ctweets %>%
  anti_join(stop_words)

clean_Ltweets_words <- clean_Ltweets %>%
  anti_join(stop_words)

```

```{r}

clean_Ctweets_words %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(y = "Count",
      x = "Unique words",
      title = "Count of unique words found in tweets",
      subtitle = "Stop words removed from the list")

```

```{r}

clean_Ltweets_words %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(y = "Count",
      x = "Unique words",
      title = "Count of unique words found in tweets",
      subtitle = "Stop words removed from the list")

```

```{r}

clean_Ctweets_paired_words <- LCtweets %>% 
  select(stripped_text) %>%
  unnest_tokens(paired_words, stripped_text, token ="ngrams", n = 2)

clean_Ctweets_paired_words %>%
  count(paired_words, sort = TRUE)

clean_Ltweets_paired_words <- LLtweets %>% 
  select(stripped_text) %>%
  unnest_tokens(paired_words, stripped_text, token ="ngrams", n = 2)

clean_Ltweets_paired_words %>%
  count(paired_words, sort = TRUE)

```


```{r}

clean_Ctweets_separated_words <- clean_Ctweets_paired_words %>% 
  separate(paired_words, c("word1", "word2"), sep = " ")

clean_Ctweets_filtered <- clean_Ctweets_separated_words %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
  
clean_Ctweets_pf_counts <- clean_Ctweets_filtered %>%
  count(word1, word2, sort = TRUE)


clean_Ltweets_separated_words <- clean_Ltweets_paired_words %>%
  separate(paired_words, c("word1", "word2"), sep = " ")

clean_Ltweets_filtered <- clean_Ltweets_separated_words %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

clean_Ltweets_pf_counts <- clean_Ltweets_filtered %>%
  count(word1, word2, sort = TRUE)

```


```{r}

clean_Ctweets_pf_counts %>%
        filter(n >= 40) %>%
        graph_from_data_frame() %>%
        ggraph(layout = "fr") +
        # geom_edge_link(aes(edge_alpha = n, edge_width = n))
        # geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
        geom_node_point(color = "darkslategray4", size = 3) +
        geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
        labs(title = "Word Network: Tweets of Conservative News",
             subtitle = "Text mining twitter data ",
             x = "", y = "")

```


```{r}

clean_Ltweets_pf_counts %>%
        filter(n >= 40) %>%
        graph_from_data_frame() %>%
        ggraph(layout = "fr") +
        # geom_edge_link(aes(edge_alpha = n, edge_width = n))
        # geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
        geom_node_point(color = "darkslategray4", size = 3) +
        geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
        labs(title = "Word Network: Tweets of Liberal News",
             subtitle = "Text mining twitter data ",
             x = "", y = "")

```



```{r}

clean_Ctweets_words %>%
  count(word,sort = T) %>%
  with( wordcloud(word, n, max.word=100))


clean_Ltweets_words %>%
  count(word,sort = T) %>%
  with( wordcloud(word, n, max.word=100))

LCtweet_words <- bind_rows(
  mutate(
    clean_Ctweets_words, Orientation="Right"
    ),
  mutate(
    clean_Ltweets_words, Orientation="Left"
  )
)
```

## Sentiment analysis

```{r}


LLtweets %>%
  select(stripped_text, screen_name, favorite_count, retweet_count, followers_count, display_text_width, created_at, is_retweet) -> Str_LLtweets

Str_LLtweets %>%
  select(stripped_text) -> left_text

write_csv(left_text, "left_text.csv")

LCtweets %>%
  select(stripped_text, screen_name, favorite_count, retweet_count, followers_count, display_text_width, created_at, is_retweet) -> Str_LCtweets

Str_LCtweets %>%
  select(stripped_text) -> right_text

write_csv(right_text, "right_text.csv")

```

```{r}


word.LL <- as.vector(Str_LLtweets$stripped_text)

word.LC <- as.vector(Str_LCtweets$stripped_text)


```

```{r}

emoitonLL <- get_nrc_sentiment(word.LL)


emotionLC <- get_nrc_sentiment(word.LC)

```

```{r}


sentLL <- cbind(Str_LLtweets, emoitonLL)

sentLL %>%
  mutate(num = row_number()) -> sentLL

sentLC <- cbind(Str_LCtweets, emotionLC)

sentLC %>%
  mutate(num = row_number()) -> sentLC

```

```{r}

ALLTs <- bind_rows(
  mutate(
     sentLL, Orientation="LL"
    ),
  mutate(
    sentLC, Orientation="LC"
  )
)

write_csv(ALLTs, "L_R_tweettext_429.csv")
```

```{r}

#syuhzuet sentiment data
ALLTs <- read_csv("L_R_tweettext_429.csv")

vader <- read_csv("vader.csv")

```

```{r}

ALLTs %>%
  mutate(sent_value = get_sentiment(stripped_text)) -> ALLTs2

```

```{r}

max(ALLTs2$sent_value)
min(ALLTs2$sent_value)

mean(ALLTs2$sent_value)
sd(ALLTs2$sent_value)
```

```{r}

ALLTs2 %>%
  group_by(Orientation) %>%
  mutate(pos_ave = mean(positive)) %>%
  mutate(pos_var = relativeSD(positive, 0, 9)) %>%
  mutate(neg_ave = mean(negative)) %>%
  mutate(neg_var = relativeSD(negative, 0, 9)) %>%
  mutate(affect = positive - negative) %>%
  ungroup() %>%
  mutate(interactions = retweet_count + favorite_count) -> Sent_media


Sent_media$Orientation <- as.factor(Sent_media$Orientation)
```

```{r}

sumdat_p <- summarySE(Sent_media, "positive", groupvars = c("screen_name", "Orientation"))
sumdat_n <- summarySE(Sent_media, "negative", groupvars = c("screen_name", "Orientation"))

sumdat_p %>%
  ggplot() + 
  geom_bar(aes(x = screen_name, y = positive_mean, fill = Orientation), stat = "identity") +
  geom_errorbar(aes(x = screen_name, y = positive_mean, group = screen_name, ymin = positive_mean-se, ymax = positive_mean+se), color = "black", width = .05)


sumdat_n %>%
  ggplot() + 
  geom_bar(aes(x = screen_name, y = negative_mean, fill = Orientation), stat = "identity") +
  geom_errorbar(aes(x = screen_name, y = negative_mean, group = screen_name, ymin = negative_mean-se, ymax = negative_mean+se), color = "black", width = .05)

```

```{r}

library(lmerTest)

```

```{r}

library(lme4)

```

```{r}

library(sjPlot)

library(multilevel)
library(sjmisc)
library(MuMIn)
library(arm)

library(ggstatsplot)

```

```{r}

Sent_mediaO <- read_csv("NewsOrgdata10142019.csv")
senti <- read_csv("result2.csv")
Sent_media2 <- read_csv("NewsOrgdata10142019_vadardata.csv")

```



```{r}

#VADER 

```

```{r}

write.csv(Sent_media, file = "NewsOrgdata10142019.csv")

```

```{r}

tweetdata <- read_csv("NewsTwitterData.csv")

```

```{r}

Sent_media$screen_name <- as.factor(Sent_media$screen_name)

Sent_mediaO %>%
  ungroup() %>%
  filter(stripped_text != ".") %>%
  filter(stripped_text != "") %>%
  filter(stripped_text != "NA") %>%
  filter(stripped_text != ",") -> Sent_media2

Sent_media2$screen_name <- as.factor(Sent_media2$screen_name)

Sent_media2 %>%
  mutate(pos_c = positive - pos_ave,
         neg_c = negative - neg_ave) -> Sent_media2 
```


```{r}
write_csv(Sent_media2, "saved_text.csv")

Sent_media2 <- read_csv("saved_text.csv")

Sent_media2 %>%
  select(stripped_text) -> text
write_csv(text, "text.csv")

Sent_media2 %>%
  mutate(sent_vader = vader$compound,
         pos_vader = vader$pos,
         neg_vader = vader$neg) -> Sent_media2 

Sent_media2 -> analysis
```

```{r}

analysis %>%
  mutate(followers_countS = scale(followers_count)) %>%
  mutate(display_text_widthS = scale(display_text_width),
         log_int = log10(interactions + 1)) -> analysis

analysis %>%
  filter(complete.cases(.)) -> analysis

analysis$Orientation <- as.factor(analysis$Orientation)
```

```{r}
library(splitstackshape)

analysis %>%
  ungroup() %>%
  mutate(rand_id = runif(140358, min = 0, max = 1)) %>%
  mutate(split_id = ifelse(rand_id >.5, 1, 0))-> analysisG

analysisG %>%
  filter(Orientation != "LC") ->analysisLL

analysisG %>%
  filter(Orientation != "LL") ->analysisLC

analysisG %>%
  filter(split_id == 1) -> split1

write_csv(split1, "split1.csv")

split1 <- read_csv("split1.csv")

analysisG %>%
  filter(split_id == 0) -> split0

write_csv(split0, "split0.csv")

split0 <- read_csv("split0.csv")
```


```{r}
split0 %>%
  group_by(screen_name) %>%
  dplyr::summarise(n = n())

analysisG %>%
  mutate(sent_vader2 = (sent_vader)^2) ->analysisG

analysis %>%
  mutate(sent_vaderc = sent_vader - mean(sent_vader),
        vaderc2 = sent_vaderc^2,
        posc = positive - mean(positive),
        posc2 = posc^2,
        negc = negative - mean(negative),
        negc2 = negc^2,
        log_int = log10(interactions + 1)) -> analysis

analysis %>%
  mutate(split_id = analysisG$split_id) -> analysis

analysis %>%
  filter(Orientation != "LC") ->analysisLL

analysis %>%
  filter(Orientation != "LL") ->analysisLC

analysis %>%
  filter(split_id == 1) -> split1


analysis %>%
  filter(split_id == 0) -> split0

cor <- lme(negative ~ neg_vader, random =~ 1|screen_name, data = analysis)
summary(cor)

cor.test(analysis$negative, analysis$neg_vader)

```

```{r}
analysis %>%
  gather(valence_type, affect, positive, negative) -> analysis_long
```

```{r}
ggstatsplot::grouped_ggwithinstats(
  data = analysis_long %>%
    dplyr::group_by(screen_name, Orientation, valence_type) %>%
    dplyr::summarise(affect = mean(affect), sd = sd(affect)),
  x = valence_type,
  y = affect,
  grouping.var = Orientation,
  xlab = "Valence Type", # label for the x-axis variable
  ylab = "Affect", # label for the y-axis variable
  title = "",
  results.subtitle = TRUE,
  point.path = FALSE)

mod <- aov(negative ~ Orientation, data = mean_test)
summary(mod)

analysis_long %>%
  dplyr::group_by(Orientation, valence_type) %>%
  dplyr::summarise(mean = mean(affect), sd = sd(affect))

summz <- summarySE(analysis_long, measurevar = "affect", groupvars = c("Orientation", "valence_type"))

summz %>%
  mutate(valence = valence_type) -> summz

summz %>%
  ggplot() +
  geom_bar(aes(x = valence, y = affect_mean, fill = Orientation), stat = "identity", position = "dodge")  +
  geom_errorbar(aes(x = valence, y = affect_mean, group = Orientation, 
         ymin = affect_mean-se, 
         ymax = affect_mean+se),position = position_dodge(.9), colour = "Black", width = .35, size = .7) +
  xlab("Valence") +
  ylab("Affect") +
  scale_fill_manual(values = c("blue", "red"),name= "Orientation") +
  theme_bw()

t.test(analysis$negative ~ analysis$Orientation)
```

#USE THIS ONE
```{r}
analysis %>%
  mutate(Orientation = ifelse(Orientation == "LC", "Right", "Left")) ->analysis

analysis$Orientation <- as.factor(analysis$Orientation)
analysis$screen_name <- as.factor(analysis$screen_name)

LL3e <- lmer(log_int ~ pos_vader * Orientation + neg_vader * Orientation + display_text_widthS + followers_countS + (1|screen_name), data = analysis, REML = FALSE)

summary(LL3e)

LL3 <- lme(log_int ~ pos_vader + neg_vader + display_text_widthS + followers_countS, random =~ 1|screen_name, data = analysisLL)
summary(LL3)

plot_model(LL3e, type = "pred", terms = c("negative", "Orientation"), colors = c("blue", "red")) +
  xlab("Negativity") +
  ylab("Interactions (log scaled)") + 
  theme_bw()

plot_model(LL3e, type = "pred", terms = c("positive", "Orientation"), colors = c("blue", "red")) +
  xlab("Positivty") +
  ylab("Interactions (log scaled)") + 
  theme_bw()

cohens_f(LL3e, ci = .95, partial = TRUE)

LC3 <- lme(log_int ~ pos_vader + neg_vader + display_text_widthS + followers_countS, random =~ 1|screen_name, data = analysisLC)
summary(LC3)

plot_model(LC3)
```


#Text Network Analysis

```{r}
analysis %>%
  mutate(val_code = ifelse(sent_vader >0, "positive", 
                           ifelse(sent_vader < 0, "negative", "neutral"))) -> net_dat
```

```{r}
net_dat %>%
  select(stripped_text, Orientation, val_code) -> net_dat
```


```{r}
#count pairwise occurences of words which appear together in the text, this is what is known as bigram count.
net_dat <- as_tibble(net_dat)

net_dat %>% select(stripped_text) -> net_dat2

```

```{r}

net_dat2 %>%
  #convert text to lower case
  mutate(Text = stripped_text %>% str_to_lower()) %>% 
  #remove unwanted characters
  mutate(Text = Text %>% str_remove_all(pattern = "\\n"),
         Text = Text %>% str_remove_all(pattern = '&amp'),
         Text = Text %>% str_remove_all(pattern = 'https://t.co/[a-z,A-Z,0-9]*'),
         Text = Text %>% str_remove_all(pattern = 'http://t.co/[a-z,A-Z,0-9]*'),
         Text = Text %>% str_remove_all(pattern = 'https'),
         Text = Text %>% str_remove_all(pattern = 'http'),
         #remove hashtags
         Text = Text %>% str_remove_all(pattern = '#[a-z,A-Z]*'),
         #remove accounts
         Text = Text %>% str_remove_all(pattern = '@[a-z,A-Z]*'),
         #remove retweets
         Text = Text %>% str_remove_all(pattern = 'rt [a-z,A-Z]*: '),
         Text = Text %>% str_remove(pattern = '^(rt)'),
         Text = Text %>% str_remove_all(pattern = '\\_')) -> potus2
         

# Replace accents. 
replacement.list <- list('á' = 'a', 'é' = 'e', 'í' = 'i', 'ó' = 'o', 'ú' = 'u')

potus2 %>% 
  mutate(Text = chartr(old = names(replacement.list) %>% str_c(collapse = ''), 
                       new = replacement.list %>% str_c(collapse = ''),
                       x = Text)) -> potus2
```

```{r}
#create clean text column in data frame
corpus <- Corpus(x = VectorSource(x = potus2$Text))

corpus %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords('en')) %>%
  tm_map(PlainTextDocument) -> potus_text

potus2 %>%
  mutate(Text_corpus = potus_text[[1]]$content) -> text_dat
```

```{r}
net_dat$Text <- text_dat$Text
net_dat$text_corpus <- text_dat$Text_corpus
```

```{r}
net_dat %>%
  filter(Orientation == "Left",
         val_code == "positive") -> net_left_pos

net_dat %>%
  filter(Orientation == "Left",
         val_code == "negative") -> net_left_neg

net_dat %>%
  filter(Orientation == "Right",
         val_code == "positive") -> net_right_pos

net_dat %>%
  filter(Orientation == "Right",
          val_code == "negative") -> net_right_neg
```


```{r}

text_dat$Text <- as.vector(text_dat$Text)

text_dat %>% 
  unnest_tokens(
    input = Text, 
    output = bigram, 
    token = 'ngrams', 
    n = 2
  ) %>% 
  filter(! is.na(bigram)) -> bigram_words


```


```{r}
#filter for stop words and remove white spaces.
stopwords.df <- tibble(
  word = c(stopwords(kind = 'en'))
)

bigram_words %>% 
  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% 
  filter(! word1 %in% stopwords.df$word) %>% 
  filter(! word2 %in% stopwords.df$word) %>% 
  filter(! is.na(word1)) %>% 
  filter(! is.na(word2)) -> bigram_words
```

```{r}
#group and count bigrams
bigram_words %>%
  count(word1, word2, sort = TRUE) %>% 
  # We rename the weight column so that the 
  # associated network gets the weights (see below).
  rename(weight = n) -> bigram_count
```

```{r}
bigram_count %>% 
  mutate(weight = log(weight + 1)) %>% 
  ggplot(mapping = aes(x = weight)) +
    theme_light() +
    geom_histogram(bins = 40) +
    labs(title = "Bigram log-Weight Distribution")
```

##How to define a weighted network from a bigram count?

###Each word represents a node.
###Two words areconnected if they appear as a bigram.
###The weight of an edge is the number of times the bigram appears in the corpus.
###(Optional) We are free to decide if we want the graph to be directed or not.
###We are going to use the igraph library to work with networks. The reference A User’s Guide to Network Analysis in R is highly recomended if you want to go deeper into network analysis in R.

###For visualization purposes, we can set a threshold which defines the minimal weight allowed in the graph.

```{r}
threshold <- 250 # the minimum number of times a bigram appears

# For visualization purposes we scale by a global factor. 
ScaleWeight <- function(x, lambda) {
  x / lambda
}

network <-  bigram_count %>%
  filter(weight > threshold) %>%
  mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>% 
  graph_from_data_frame(directed = FALSE)
```

```{r}
# verify we have a weighted network
is.weighted(network)
```

```{r}
 plot(
  network, 
  vertex.size = 1,
  vertex.label.color = 'black', 
  vertex.label.cex = 0.7, 
  vertex.label.dist = 1,
  edge.color = 'gray', 
  main = 'Bigram Count Network', 
  sub = glue('Weight Threshold: {threshold}'), 
  alpha = 50
)
```

```{r}
# Store the degree.
V(network)$degree <- strength(graph = network)

# Compute the weight shares.
E(network)$width <- E(network)$weight/max(E(network)$weight)

plot(
  network, 
  vertex.color = 'lightblue',
  # Scale node size by degree.
  vertex.size = 2*V(network)$degree,
  vertex.label.color = 'black', 
  vertex.label.cex = 0.6, 
  vertex.label.dist = 1.6,
  edge.color = 'gray', 
  # Set edge width proportional to the weight relative value.
  edge.width = 10*E(network)$width ,
  main = 'Bigram Count Network', 
  sub = glue('Weight Threshold: {threshold}'), 
  alpha = 50
)
```

```{r}
# Get all connected components.
clusters(graph = network)
```

```{r}
# Select biggest connected component.  
V(network)$cluster <- clusters(graph = network)$membership

cc.network <- induced_subgraph(
  graph = network,
  vids = which(V(network)$cluster == which.max(clusters(graph = network)$csize))
)
```

```{r}
# Store the degree.
V(cc.network)$degree <- strength(graph = cc.network)

# Compute the weight shares.
E(cc.network)$width <- E(cc.network)$weight/max(E(cc.network)$weight)

 plot(
  cc.network, 
  vertex.color = 'lightblue',
  # Scale node size by degree.
  vertex.size = 2*V(cc.network)$degree,
  vertex.label.color = 'black', 
  vertex.label.cex = 0.6, 
  vertex.label.dist = 1.6,
  edge.color = 'gray', 
  # Set edge width proportional to the weight relative value.
  edge.width = 3*E(cc.network)$width ,
  main = 'Bigram Count Network (Biggest Connected Component)', 
  sub = glue('Weiight Threshold: {threshold}'), 
  alpha = 50
)
```

```{r}
threshold <- 250 # the minimum number of times a bigram appears

network <-  bigram_count %>%
  filter(weight > threshold) %>%
  mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>% 
  graph_from_data_frame(directed = FALSE)

# Store the degree.
V(network)$degree <- strength(graph = network)
# Compute the weight shares.
E(network)$width <- E(network)$weight/max(E(network)$weight)

#the networkD3 library can make these visuals more dynamic
# Create networkD3 object.
network.D3 <- igraph_to_networkD3(g = network)
# Define node size.
network.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)
# Degine color group (I will explore this feature later).
network.D3$nodes %<>% mutate(Group = 1)
# Define edges width. 
network.D3$links$Width <- 10*E(network)$width

forceNetwork(
  Links = network.D3$links, 
  Nodes = network.D3$nodes, 
  Source = 'source', 
  Target = 'target',
  NodeID = 'name',
  Group = 'Group', 
  opacity = 0.9,
  Value = 'Width',
  Nodesize = 'Degree', 
  # We input a JavaScript function.
  linkWidth = JS("function(d) { return Math.sqrt(d.value); }"), 
  fontSize = 12,
  zoom = TRUE, 
  opacityNoHover = 1
)
```
#Skimgram Analysis - allows for a "jump in the word count
```{r}
skip.window <- 2

skip.gram.words <- text_dat %>% 
  unnest_tokens(
    input = Text, 
    output = skipgram, 
    token = 'skip_ngrams', 
    n = skip.window
  ) %>% 
  filter(! is.na(skipgram))

```

```{r}
#count the skipgrams
skip.gram.words$num_words <- skip.gram.words$skipgram %>% 
  map_int(.f = ~ ngram::wordcount(.x))

skip.gram.words %<>% filter(num_words == 2) %>% select(- num_words)

skip.gram.words %<>% 
  separate(col = skipgram, into = c('word1', 'word2'), sep = ' ') %>% 
  filter(! word1 %in% stopwords.df$word) %>% 
  filter(! word2 %in% stopwords.df$word) %>% 
  filter(! is.na(word1)) %>% 
  filter(! is.na(word2)) 

skip.gram.count <- skip.gram.words  %>% 
  count(word1, word2, sort = TRUE) %>% 
  rename(weight = n)

skip.gram.count %>% head()
```

```{r}
#visualize skipgrams
# Treshold
threshold <- 10

network <-  skip.gram.count %>%
  filter(weight > threshold) %>%
  graph_from_data_frame(directed = FALSE)

# Select biggest connected component.  
V(network)$cluster <- clusters(graph = network)$membership

cc.network <- induced_subgraph(
  graph = network,
  vids = which(V(network)$cluster == which.max(clusters(graph = network)$csize))
)

# Store the degree.
V(cc.network)$degree <- strength(graph = cc.network)
# Compute the weight shares.
E(cc.network)$width <- E(cc.network)$weight/max(E(cc.network)$weight)

# Create networkD3 object.
network.D3 <- igraph_to_networkD3(g = cc.network)
# Define node size.
network.D3$nodes %<>% mutate(Degree = (1E-2)*V(cc.network)$degree)
# Degine color group (I will explore this feature later).
network.D3$nodes %<>% mutate(Group = 1)
# Define edges width. 
network.D3$links$Width <- 10*E(cc.network)$width

forceNetwork(
  Links = network.D3$links, 
  Nodes = network.D3$nodes, 
  Source = 'source', 
  Target = 'target',
  NodeID = 'name',
  Group = 'Group', 
  opacity = 0.9,
  Value = 'Width',
  Nodesize = 'Degree', 
  # We input a JavaScript function.
  linkWidth = JS("function(d) { return Math.sqrt(d.value); }"), 
  fontSize = 12,
  zoom = TRUE, 
  opacityNoHover = 1
)
```

```{r}
#Node importance 
#There are many notions of node importance in a network (A User’s Guide to Network Analysis in R, Section 7.2). Here we compare three of them

#Degree centrality - the number of links to other nodes an indiviudal node has

#Closeness centrality - average length of the shortest path between the node and all other nodes

#Betweenness centrality - the number of times a node acts as a bridge between other nodes. High scores mean it connects lots of other nodes.


# Compute the centrality measures for the biggest connected component from above.
node.impo.df <- tibble(
  word = V(cc.network)$name,  
  degree = strength(graph = cc.network),
  closeness = closeness(graph = cc.network), 
  betweenness = betweenness(graph = cc.network)
)
```

```{r}
#rank nodes based on the centrality measures
node.impo.df %>% 
  arrange(- degree) %>%
  head(10)

node.impo.df %>% 
  arrange(- closeness) %>%
  head(10)

node.impo.df %>% 
  arrange(- betweenness) %>% 
  head(10)
```

```{r}
#visualize the centrality measures
plt.deg <- node.impo.df %>% 
  ggplot(mapping = aes(x = degree)) +
    theme_light() +
    geom_histogram(fill = 'blue', alpha = 0.8, bins = 30)

plt.clo <- node.impo.df %>% 
  ggplot(mapping = aes(x = closeness)) +
    theme_light() +
    geom_histogram(fill = 'red', alpha = 0.8, bins = 30)

plt.bet <- node.impo.df %>% 
  ggplot(mapping = aes(x = betweenness)) +
    theme_light() +
    geom_histogram(fill = 'green4', alpha = 0.8, bins = 30)

plot_grid(
  ... = plt.deg, 
  plt.clo, 
  plt.bet, 
  ncol = 1, 
  align = 'v'
)
```

```{r}
#identify clusters using Louvain Method for community detection:

comm.det.obj <- cluster_louvain(
  graph = cc.network, 
  weights = E(cc.network)$weight
)

comm.det.obj

#7 groups or "clusters" were identified. 
#mod (modularity) - is .75 which is good (closer to 1 is better)
##Modularity is as chance-corrected statistic, and is defined as the fraction of ties that fall within the given groups minus the expected such fraction if the ties were distributed at random.
```

```{r}
#Now we encode the membership as a node atribute (zoom and click on each node to explore the clusters).
V(cc.network)$membership <- membership(comm.det.obj)

# We use the membership label to color the nodes.
network.D3$nodes$Group <- V(cc.network)$membership

forceNetwork(
  Links = network.D3$links, 
  Nodes = network.D3$nodes, 
  Source = 'source', 
  Target = 'target',
  NodeID = 'name',
  Group = 'Group', 
  opacity = 0.9,
  Value = 'Width',
  Nodesize = 'Degree', 
  # We input a JavaScript function.
  linkWidth = JS("function(d) { return Math.sqrt(d.value); }"), 
  fontSize = 12,
  zoom = TRUE, 
  opacityNoHover = 1
)
```

```{r}
#Collect the words per cluster:
membership.df <- tibble(
  word = V(cc.network) %>% names(),
  cluster = V(cc.network)$membership
)

V(cc.network)$membership %>%
  unique %>% 
  sort %>% 
  map_chr(.f = function(cluster.id) {
    
    membership.df %>% 
      filter(cluster == cluster.id) %>% 
      # Get 15 at most 15 words per cluster.
      slice(1:15) %>% 
      pull(word) %>% 
      str_c(collapse = ', ')
    
  }) 
```

#Correlation Analysis (Phi Coefficient)
##Network Definition
```{r}
#The focus of the phi coefficient is how much more likely it is that either both word X and Y appear, or neither do, than that one appears without the other. (Text Mining with R, Section 4.2.2).

words.df %>% 
  group_by(word) %>% 
  filter(n() > 5) %>% 
  pairwise_cor(item = word, feature = status_id)-> cor_words
```

```{r}
#visualize the correlation between two important nodes
topic.words <- c('news', 'fake') #set the nodes

# Set correlation threshold. 
threshold = 0.1

network <- cor_words %>%
  rename(weight = correlation) %>% 
  # filter for relevant nodes.
  filter((item1 %in% topic.words | item2 %in% topic.words)) %>% 
  filter(weight > threshold) %>%
  graph_from_data_frame()
  
V(network)$degree <- strength(graph = network)

E(network)$width <- E(network)$weight/max(E(network)$weight)

network.D3 <- igraph_to_networkD3(g = network)

network.D3$nodes %<>% mutate(Degree = 5*V(network)$degree)

# Define color groups. 
network.D3$nodes$Group <- network.D3$nodes$name %>% 
  as.character() %>% 
  map_dbl(.f = function(name) {
    index <- which(name == topic.words) 
    ifelse(
      test = length(index) > 0,
      yes = index, 
      no = 0
    )
  }
)

network.D3$links %<>% mutate(Width = 10*E(network)$width)

forceNetwork(
  Links = network.D3$links, 
  Nodes = network.D3$nodes, 
  Source = 'source', 
  Target = 'target',
  NodeID = 'name',
  Group = 'Group', 
  # We color the nodes using JavaScript code.
  colourScale = JS('d3.scaleOrdinal().domain([0,1,2]).range(["gray", "blue", "red", "black"])'), 
  opacity = 0.8,
  Value = 'Width',
  Nodesize = 'Degree', 
  # We define edge properties using JavaScript code.
  linkWidth = JS("function(d) { return Math.sqrt(d.value); }"), 
  linkDistance = JS("function(d) { return 550/(d.value + 1); }"), 
  fontSize = 18,
  zoom = TRUE, 
  opacityNoHover = 1
)
```




#Old Cluster Analysis Code


```{r}
library(colorspace)
library(lsa)
library(nFactors)

analysisG %>%
  dplyr::select(anger, anticipation, disgust, fear, joy, sadness, surprise, trust, negative, positive) -> cluster_data
```

```{r}
scores_a <- scale(cluster_data)

```


##  Calculate a distance matrix
```{r}
# To choose a distance matrix calculation put one of the following in the - method = "   "  - section.
# euclidean = euclidean distance
# manhattan = city block

set.seed(123)
gc()
score_distances <- dist(scores_a,method="euclidean")


```

##  Run the cluster analysis
```{r}
# To choose a clustering method, place your chosen method in 'method = "ward.D'
# single linkage: method = "single"
# complete linkage: method = "complete"
# average linkage: method = "average" (= UPGMA)
# Ward's method: method = "ward.D"
appraisal_cluster <- hclust(appraisal_distances,method = "ward.D")



```

##  Determine number of clusters with parallel analysis
```{r}
# Get eigenvalues
ev_a <- eigen(cor(scores_a))

# Run parallel analysis 100 times, get output
ap_a <- parallel(subject=nrow(scores_a),var=ncol(scores_a),rep=100,cent=.05)

```

##  View the results of the parallel analysis
```{r}
nS_a <- nScree(x=ev_a$values,aparallel=ap_a$eigen$qevpea)
plotnScree(nS_a)


```


##  Plot the dendogram
```{r}
plot(appraisal_cluster, hang = -1)
cluster_colors <- rev(rainbow_hcl(2))
rect.hclust(appraisal_cluster, k = 4, border = cluster_colors)
```


##  Assign to clusters
```{r}
#appraisal profiles
cluster_assignments_a <- cutree(appraisal_cluster, 4)

analysis %>%
  mutate(appraisal_profile = cluster_assignments_a) -> analysis

analysis$appraisal_profile <- as.factor(analysis$appraisal_profile)

```

##  Cluster the characteristics
```{r}
characteristic_distances_a <- dist(t(scores_a),method="euclidean")
characteristic_cluster_a <- hclust(characteristic_distances_a,method = "ward.D")
plot(characteristic_cluster_a,hang=-1)
cluster_colors <- rev(rainbow_hcl(2))
rect.hclust(characteristic_cluster_a, k = 6, border = cluster_colors)
```


##  Create a heatmap
```{r}
library(gplots)

heatmap.2(as.matrix(scores_a),Rowv = as.dendrogram(appraisal_cluster),Colv = as.dendrogram(characteristic_cluster_a),trace="none",cexCol=.5,cexRow=.5)

```






























